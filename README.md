# LLM-Response-Risk-Analyzer
An evaluation tool that benchmarks LLM responses to known prompt-injection patterns and classifies potential safety and policy risks using an automated judging model.
